---
title: "A primer on mechanistic inference with differentiable process-based models in Julia"
jupyter: julia-1.10
execute: 
    eval: false
showsol: true
format:
    html:
        toc: true
        toc-depth: 4
        # toc-expand: 2
editor:
  render-on-save: true
---


Here you will learn about different techniques to infer parameters of a (differentiable) process-based model against data. This is useful to in the context of mechanistic inference, where we want to explain patterns in a system by understanding the processes that generate them, in contrast to purely statistical or empirical inference, which might identify patterns or correlations in data without necessarily understanding the causes. We'll mostly focus on differential equation models. Make sure that you stick to the end, where we'll see how we can not only infer parameter values but also functional forms, by parametrizing models' components with neural networks.

# Preliminaries
## Wait, what is a differentiable model?
One can usually write a model as a map $\mathcal{M}$ mapping some parameters $p$, an initial state $u_0$ and a time $t$ to a future state $u_t$

$$u_t = \mathcal{M}(u_0, t, p).$$

We call **differentiable** a model $\mathcal{M}$ for which we can calculate its derivative with respect to $p$ or $u_0$. The derivative $\frac{\partial \mathcal{M}}{\partial \theta}$ expresses how much the model output changes with respect to a small change in $\theta$.

::: {.callout-note}

# Recall your Calculus class!

$$\frac{df}{dx}(x) = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}$$
:::

Let's illustrate this concept with the [logistic equation model](https://en.wikipedia.org/wiki/Logistic_function#In_ecology:_modeling_population_growth).
This model has an analytic formulation given by:

$$\mathcal{M}(u_0, p, t) = \frac{K}{1 + \big( \frac{K-u_0}{u_0} \big) e^{rt}}$$

Let's code it

```{julia}
using UnPack
using Plots
using Random
using ComponentArrays
using BenchmarkTools
Random.seed!(0)

function mymodel(u0, p, t)
    T = eltype(u0)
    @unpack r, K = p

    @. K / (one(T) + (K - u0) / u0 * exp(-r * t))
end

p = ComponentArray(;r = 1., K = 1.)
u0 = 0.005

tsteps = range(0, 20, length=100)
y = mymodel(u0, p, tsteps)

plot(tsteps, y)
```

::: {.callout-note}

# What is a `ComponentArray`?
A `ComponentArray` is a convenient Array type that allows to access array elements with symbols, similarly to a `NamedTuple`, while behaving like a standard array. For instance, you could do something like

```{julia}
cv = ComponentVector(;a = 1, b = 2)
cv .= [3, 4]
```
This is useful, because you can only calculate a gradient w.r.t a `Vector`!

:::


Now let's try to calculate the gradient of this model. While you could in this case derive the gradient analytically, an analytic derivation is generally tricky with complex models. And what about models that can only be simulated numerically, with no analytic expressions? We need to find a more automatized way to calculate gradients. 

How about [the finite difference method](https://en.wikipedia.org/wiki/Finite_difference_method)?




::: {.callout-tip icon=false}
# Exercise

Implement the function `∂mymodel_∂K(h, u0, p, t)` which returns the model's derivative with respect to `K`, calculated with a small `h` to be provided by the user.

::: {.content-hidden unless-meta="showsol"}
```{julia}
function ∂mymodel_∂K(h, u0, p, t)
    phat = (; r = p.r, K= p.K + h)
    return (mymodel(u0, phat, t) - mymodel(u0, p, t)) / h
end

∂mymodel_∂p(1e-1, u0, p, 1.)
```
:::

::: {.content-visible unless-meta="showsol"}
```{julia}
#...
```
:::

:::

The gradient of the model is useful to understand how a parameter influences the output of the model. Let's calculate the importance of the carrying capacity `K` on the model output:

```julia
dm_dp = ∂mymodel_∂p(1e-1, u0, p, tsteps)
plot(tsteps, dm_dp)
```

As you can observe, the carrying capacity has no effect at small $t$ where population is small, and its influence on the dynamics grows as the population grows. We expect the reverse effect for $r$.

## On the importance of gradients for inference

The ability to calculate the derivative of a model is crucial when it comes to inference. Both within a full Bayesian inference context, where one wants to sample the posterior distribution of parameters $\theta$ given data $u$, $p(\theta| u)$, or when one wants to obtain a point estimate $\theta^\star = \argmax_\theta (p(\theta | u))$ (frequentist or machine learning context), the model gradient proves very useful. In a full Bayesian inference context, they are used e.g. with Hamiltonian Markov Chains methods, such as the NUTS sampler, and in a machine learning context, they are used with gradient-based optimizer.


### Gradient descent
The best way to grasp the importance of gradients in inference is to understand the [gradient descent algorithm](https://en.wikipedia.org/wiki/Gradient_descent).


The following picture illustrates the algorithm in the special case where $p$ is one-dimensional.

![](https://editor.analyticsvidhya.com/uploads/631731_P7z2BKhd0R-9uyn9ThDasA.png)

Given an initial estimate of the parameter value $p_0$, $\frac{d \mathcal{M}}{dp}$ is used to suggest a new, better estimate, following

$$p_{n+1} = p_n - \eta \frac{d \mathcal{M}}{dp}(u_0, t, p) $$

where $\eta$ is the learning rate.

Gradient-based methods are usually very efficient in high-dimensional spaces. 

## Automatic differentiation
Let's go back to our method `∂mymodel_∂p`.
What is the optimal value of `h` to calculate the derivative? This is a tricky question, because a too small `h` can lead to round off errors ([see more explanations here](https://book.sciml.ai/notes/08-Forward-Mode_Automatic_Differentiation_(AD)_via_High_Dimensional_Algebras/)) while `h` too large also leads to a bad approximation of the asymptotic definition.

<!-- Also, can you calculate how many evaluations of the model do you need if your parameter is $d$ dimensionsal?
$\mathcal{O}(2 d)$ -->

Fortunately, a bunch of techniques referred to as [**automatic differentiation**](https://en.wikipedia.org/wiki/Automatic_differentiation) (AD) allows to **exactly** differentiate any piece of numerical functions. In practice, your code must be exclusively written within an AD-backend, such as Torch, JAX or Tensorflow. Those libraries do not know how to differentiate code not written in their own language, such as normal Python code.

Fortunately, Julia is an *AD-pervasive language*! This means that any piece of Julia code is theoretically differentiable with AD.

```{julia}
using ForwardDiff

@btime ForwardDiff.gradient(p -> mymodel(u0, p, 1.), p)
```

This is what makes Julia great for model calibration and inference! Write your model in Julia, and any inference method using AD will be able to work with your model!

To learn more about AD in Julia, check-out this [cool blog-post](https://gdalle.github.io/AutodiffTutorial/) and [this short presentation](https://gdalle.github.io/JuliaCon2024-AutoDiff/#/title-slide).

Now let's get started with inference.

# Mechanistic inference

## The mechanistic model and the data
We'll use a simple dynamical community model, the [Lotka Volterra](https://en.wikipedia.org/wiki/Lotka–Volterra_equations) model, to generate data. We'll then contaminate this data with noise, and try to recover the parameters that have generated the data. The goal of the session will be to estimate those parameters from the data, using a bunch of different techniques. 

So let's first generate the data.

```julia
using OrdinaryDiffEq

# Define Lotka-Volterra model.
function lotka_volterra(du, u, p, t)
    # Model parameters.
    @unpack α, β, γ, δ = p
    # Current state.
    x, y = u

    # Evaluate differential equations.
    du[1] = (α - β * y) * x # prey
    du[2] = (δ * x - γ) * y # predator

    return nothing
end

# Define initial-value problem.
u0 = [2.0, 2.0]
p_true = (;α = 1.5, β = 1.0, γ = 3.0, δ = 1.0)
# tspan = (hudson_bay_data[1,:t], hudson_bay_data[end,:t])
tspan = (0., 5.)
saveat = 0.1
alg = Tsit5()

prob = ODEProblem(lotka_volterra, u0, tspan, p_true)

sol_true = solve(prob, alg; saveat)
# Plot simulation.
plot(sol_true)
```

This is the true state of the system. Now let's contaminate it with observational noise. 


:::::: {.callout-tip icon="false"}

# Exercise: contaminate data with noise

Create a `data_mat` array, which consists of the ODE solution contaminated with a lognormally-distributed noise with standard deviation `0.3`. 

::: {.callout-note}
Note that we add lognormally-distributed noise instead of normally-distributed because we are observing population abundance, which can only be positive.
:::

::: {.content-hidden unless-meta="showsol"}
```{julia}
data_mat = Array(sol_true) .* exp.(0.3 * randn(size(sol_true)))
# Plot simulation and noisy observations.
plot(sol_true; alpha=0.3)
scatter!(sol_true.t, data_mat'; color=[1 2], label="")
```
:::

::: {.content-visible unless-meta="showsol"}
```{julia}
# data_mat = ...
```
:::
:::

Now that we have our data, let's do some inference!

## Mechanistic inference as a supervised learning task
We'll get started with a very crude approach to inference, where we'll treat the calibration of our LV model similarly to a supervised machine learning task. To do so, we'll write a loss function, defining a distance between our model and the data, and we'll try to minimize this loss. The parameter minimizing this loss will be our best model parameter estimate.

```julia
function loss(p)
    predicted = solve(prob,
                        alg; 
                        p, 
                        saveat,
                        abstol=1e-6, 
                        reltol = 1e-6)

    l = 0.
    for i in 1:length(predicted)
        if all(predicted[i] .> 0)
            l += sum(abs2, log.(data_mat[:, i]) - log.(predicted[i]))
        end
    end
    return l, predicted
end
```

::: {.callout-note}
Notice that we explicitly check whether predictions are > 0, because the log of negative number is not defined and will throw an error! 
:::

Let's define a helper function, that will plot how good does the model perform across different iterations.

```julia
losses = []
callback = function (p, l, pred; doplot=true)
    push!(losses, l)
    if length(losses)%10==0
      println("Current loss after $(length(losses)) iterations: $(losses[end])")
    end
    if doplot
        plt = scatter(sol_true.t, data_mat[1,:]; label = "data x", color = :blue, markerstrokewidth=0)
        scatter!(plt, sol_true.t, Array(pred)[1,:]; label = "prediction x", color = :blue, markershape=:star5, markerstrokewidth=0)
        scatter!(plt, sol_true.t, data_mat[2,:]; label = "data y", color = :red, markerstrokewidth=0)
        scatter!(plt, sol_true.t, Array(pred)[2,:]; label = "prediction y", color = :red, markershape=:star5, markerstrokewidth=0)
        display(plot(plt, yaxis = :log10))
    end
    return false
end

```
And let's define a wrong initial guess for the parameters

```julia
pinit = ComponentArray(;α = 1., β = 1.5, γ = 1.0, δ = 0.5)

callback(pinit, loss(pinit)...; doplot = true)
```
Our initial predictions are bad, but you'll likely get even worse predictions in a real-case scenario!

We'll use the library `Optimization`, which is a wrapper library around many optimization libraries in Julia providing you with many different types of optimizers, to find parameters minimizing `loss`. We'll specifically use the infamous [Adam optimizer](https://arxiv.org/abs/1412.6980) (187k citations!!!), widely used in ML.

```julia
using Optimization
using OptimizationOptimisers
using SciMLSensitivity

adtype = Optimization.AutoZygote()
optf = Optimization.OptimizationFunction((x, p) -> loss(x), adtype)
optprob = Optimization.OptimizationProblem(optf, pinit)

@time res_ada = Optimization.solve(optprob, ADAGrad(0.1); callback = callback, maxiters = 500)
res_ada.minimizer
```

Nice! It seems that the optimizer did a reasonable job, and that we found a reasonable estimate of our parameters!


::: {.callout-tip icon="false"}
# Exercise: Hey, this is cheating!

Notice that we use the true `u0`, as if we were to know exactly the initial state. In a real situation, we need also to infer the true state!

Can you modify the model to infer the true state?

::: {.content-hidden unless-meta="showsol"}
```{julia}
function loss2(p)
    predicted = solve(prob,
                        alg; 
                        p,
                        u0 = p.u0,
                        saveat,
                        abstol=1e-6, 
                        reltol = 1e-6)

    l = 0.
    for i in 1:length(predicted)
        if all(predicted[i] .> 0)
            l += sum(abs2, log.(data_mat[:, i]) - log.(predicted[i]))
        end
    end
    return l, predicted
end
losses = []

pinit = ComponentArray(;α = 1., β = 1.5, γ = 1.0, δ = 0.5, u0 = data_mat[:,1])

adtype = Optimization.AutoZygote()
optf = Optimization.OptimizationFunction((x, p) -> loss2(x), adtype)
optprob = Optimization.OptimizationProblem(optf, pinit)

@time res_ada = Optimization.solve(optprob, ADAGrad(0.1); callback = callback, maxiters = 1000)
res_ada.minimizer
```
:::
::: {.content-visible unless-meta="showsol"}
```{julia}
# function loss2(p)
#    ...
# end
# losses = []

# pinit = ...

# adtype = Optimization.AutoZygote()
# optf = Optimization.OptimizationFunction((x, p) -> loss2(x), adtype)
# optprob = Optimization.OptimizationProblem(optf, pinit)

# @time res_ada = Optimization.solve(optprob, ADAGrad(0.1); callback = callback, maxiters = 1000)
# res_ada.minimizer
```
:::
:::


## Sensitivity methods
Did you wonder why do we need to load `SciMLSensitivity`? and why did we specify `adtype = Optimization.AutoZygote()`? 

AD comes in different flavours, with broadly two types of AD methods - **forward methods** and **backward methods** -, and a bunch of different implementations. 

You can specify which ones `Optimization.jl` will use to differentiate `loss` with `adtype`, see available options [here](https://docs.sciml.ai/Optimization/stable/API/ad/). 

But when it comes to differentiating the `solve` function from `OrdinaryDiffEq`, you want to use `AutoZygote()`, because when trying to differentiate `solve`, a specific adjoint rule provided by the `SciMLSensitivity` package will be used. These adjoint rules can be specificed by the keyword `sensealg` when calling `solve` and are designed for best performance when differentiating numerical solutions to ODEs. There exist a lot of them (see a review [here](https://arxiv.org/abs/2406.09699)), and if `sensealg` is not provided, a smart polyalgorithm is going to pick up one for you.

You can have a look in the documentation [here](https://docs.sciml.ai/SciMLSensitivity/stable/manual/differential_equation_sensitivities/) for hints on how to choose an algorithm. 

::: {.callout-tip icon="false"}
# Exercise: benchmarking sensitivity methods

Can you evaluate the performance of `ForwardDiffSensitivity()` and `ReverseDiffAdjoint()`?

::: {.content-hidden unless-meta="showsol"}
```{julia}
using Zygote
function loss_sensealg(p, sensealg)
    predicted = solve(prob,
                        alg; 
                        sensealg,
                        p,
                        u0 = p.u0,
                        saveat,
                        abstol=1e-6, 
                        reltol = 1e-6)

    l = 0.
    for i in 1:length(predicted)
        if all(predicted[i] .> 0)
            l += sum(abs2, log.(data_mat[:, i]) - log.(predicted[i]))
        end
    end
    return l
end
```
```{julia}
@btime Zygote.gradient(p -> loss_sensealg(p, ForwardDiffSensitivity()), pinit)
```

```{julia}
@btime Zygote.gradient(p -> loss_sensealg(p, ReverseDiffAdjoint()), pinit)
```
:::
::: {.content-visible unless-meta="showsol"}
```{julia}
#...
```
:::

For a small number of parameters, forward methods tend to perform best, but with higher number of parameters, the other way around is true.

:::

Well done! Now, let's jump into the Bayesian world...

## Bayesian inference

Julia has a very strong library for Bayesian inference: [Turing.jl](https://turinglang.org).

Let's declare our first Turing model!

This is done with the `@model` macro, which allows the library to automatically construct the posterior distribution based on the definition of your model's random variables, assigned with the `~` symbol.

### Our first Turing model

```julia
using Turing
using LinearAlgebra

@model function fitlv(data, prob)
    # Prior distributions.
    σ ~ InverseGamma(3, 0.5)
    α ~ truncated(Normal(1.5, 0.5); lower=0.5, upper=2.5)
    β ~ truncated(Normal(1.2, 0.5); lower=0, upper=2)
    γ ~ truncated(Normal(3.0, 0.5); lower=1, upper=4)
    δ ~ truncated(Normal(1.0, 0.5); lower=0, upper=2)

    # Simulate Lotka-Volterra model. 
    p = (;α, β, γ, δ)
    predicted = solve(prob, alg; p, saveat)

    # Observations.
    for i in 1:length(predicted)
        if all(predicted[i] .> 0)
            data[:, i] ~ MvLogNormal(log.(predicted[i]), σ^2 * I)
        end
    end

    return nothing
end
```

Now we can instantiate our model, and run the inference!

```julia

model = fitlv(data_mat, prob)

# Sample 3 independent chains with forward-mode automatic differentiation (the default).
chain = sample(model, NUTS(), MCMCThreads(), 1000, 3; progress=true)

```
::: {.callout-note}
# Threads

How many threads do you have running? `Threads.nthreads()` will tell you!
:::


Let's see if our chains have converged.

```julia
using StatsPlots
plot(chain)
```
<!-- VICTOR, you stopped here -->
### Data retrodiction

Let's now generate simulated data using samples from the posterior distribution, and compare to the original data.

```julia
function plot_predictions(chain, sol, data_mat)
    myplot = plot(; legend=false)
    posterior_samples = sample(chain[[:α, :β, :γ, :δ]], 300; replace=false)
    for parr in eachrow(Array(posterior_samples))
        p = NamedTuple([:α, :β, :γ, :δ] .=> parr)
        sol_p = solve(prob, Tsit5(); p, saveat)
        plot!(sol_p; alpha=0.1, color="#BBBBBB")
    end

    # Plot simulation and noisy observations.
    plot!(sol; color=[1 2], linewidth=1)
    scatter!(sol.t, data_mat'; color=[1 2])
    return myplot
end
plot_predictions(chain, sol_true, data_mat)
```

### Exercise: Hey, this is cheating!

Notice that we use the true `u0`, as if we were to know exactly the initial state. In a real situation, we need also to infer the true state!

Can you modify the model to infer the true state?

```julia
@model function fitlv2(data, prob)
    # Prior distributions.
    σ ~ InverseGamma(2, 3)
    α ~ truncated(Normal(1.5, 0.5); lower=0.5, upper=2.5)
    β ~ truncated(Normal(1.2, 0.5); lower=0, upper=2)
    γ ~ truncated(Normal(3.0, 0.5); lower=1, upper=4)
    δ ~ truncated(Normal(1.0, 0.5); lower=0, upper=2)
    u0 ~ MvLogNormal(data[:,1], σ^2 * I)

    # Simulate Lotka-Volterra model but save only the second state of the system (predators).
    p = (;α, β, γ, δ)
    predicted = solve(prob, alg; p, u0, saveat)

    # Observations.
    for i in 2:length(predicted)
        if all(predicted[i] .> 0)
            data[:, i] ~ MvLogNormal(log.(predicted[i]), σ^2 * I)
        end
    end

    return nothing

end

model2 = fitlv2(data_mat, prob)

# Sample 3 independent chains.
chain2 = sample(model2, NUTS(), MCMCThreads(), 3000, 3; progress=true)
plot(chain2)
```

Here is a small utility function to visualize your results.

```julia

function plot_predictions2(chain, sol, data_mat)
    myplot = plot(; legend=false)
    posterior_samples = sample(chain, 300; replace=false)
    for i in 1:length(posterior_samples)
        ps = posterior_samples[i]
        p = get(ps, [:α, :β, :γ, :δ], flatten=true)
        u0 = get(ps, :u0, flatten = true)
        u0 = [u0[1][1], u0[2][1]]

        sol_p = solve(prob, Tsit5(); u0, p, saveat)
        plot!(sol_p; alpha=0.1, color="#BBBBBB")
    end

    # Plot simulation and noisy observations.
    plot!(sol; color=[1 2], linewidth=1)
    scatter!(sol.t, data_mat'; color=[1 2])
    return myplot
end

plot_predictions2(chain2, sol_true, data_mat)
```


### Mode estimation
Turing allows you to find the maximum likelihood estimate (MLE) or maximum a posteriori estimate (MAP), similarly to what we have done by hand with the Optimization.jl library. These can be obtained by `maximum_likelihood` and `maximum_a_posteriori`.


```julia
Random.seed!(0)
maximum_a_posteriori(model2, maxiters = 1000)
```
Since `Turing` uses under the hood the same Optimization.jl library, you can specify which optimizer youd'd like to use.

```julia
map_res = maximum_a_posteriori(model2, Adam(0.01), maxiters=2000)
```
We can check whether the optimization has converged:

```julia
@show map_res.optim_result
```
What's very nice is that Turing.jl provides you with utility functions to analyse your mode estimation results.

```julia
using StatsBase
coeftable(map_res)
```


### Partially observed state
Let's assume an ever more complicated situation: for some reason, you only have observation data for the predator. Could you still infer all parameters of your model, including those of the prey?

YES! Because the signal of the variation in abundance of the predator contains information on the dynamics of the whole predator-prey system.

Let's see how we can do that with Turing.jl. Here we need to assume so prior state for the prey. We'll just assume that it is the same as that of the predator.

```julia
@model function fitlv3(data::AbstractVector, prob)
    # Prior distributions.
    σ ~ InverseGamma(2, 3)
    α ~ truncated(Normal(1.5, 0.5); lower=0.5, upper=2.5)
    β ~ truncated(Normal(1.2, 0.5); lower=0, upper=2)
    γ ~ truncated(Normal(3.0, 0.5); lower=1, upper=4)
    δ ~ truncated(Normal(1.0, 0.5); lower=0, upper=2)
    u0 ~ MvLogNormal([data[1], data[1]], σ^2 * I)

    # Simulate Lotka-Volterra model but save only the second state of the system (predators).
    p = (;α, β, γ, δ)
    predicted = solve(prob, Tsit5(); p, u0, saveat, save_idxs=2)

    # Observations of the predators.
    for i in 2:length(predicted)
        if predicted[i] > 0
            data[i] ~ LogNormal(log.(predicted[i]), σ^2)
        end
    end

    return nothing
end

model3 = fitlv3(data_mat[2, :], prob)

# Sample 3 independent chains.
chain3 = sample(model3, NUTS(), MCMCThreads(), 3000, 3; progress=true)
plot(chain3)
p = plot_predictions2(chain3, sol, data_mat)
plot!(p, yaxis=:log10)
```

How cool!

Now you need to realise that up to now, we had a relatively simple model. How would this model scale, should we have a much larger model? Let's cook-up some idealised LV model.


### AD backends and `sensealg`
The `NUTS` sampler uses automatic differentiation under the hood. 

By default, `Turing.jl` uses `ForwardDiff.jl` as an AD backend, meaning that the SciML sensitivity methods are not used when the `solve` function is called. However, you could change the AD backend to `Zygote` with `adtype=AutoZygote()`.

```julia
chain2 = sample(model2, NUTS(), MCMCThreads(), adtype=AutoZygote(), 3000, 3; progress=true)
```

See [here](https://turinglang.org/docs/tutorials/docs-10-using-turing-autodiff/index.html) for more information. 

### Using Variational Inference

```julia
import Flux
using Turing: Variational
model = fitlv2(data_mat, prob)
q0 = Variational.meanfield(model)
advi = ADVI(10, 10_000)

q = vi(model, advi, q0; optimizer=Flux.ADAM(1e-2))

function plot_predictions_vi(q, sol, data_mat)
    myplot = plot(; legend=false)
    z = rand(q, 300)
    for parr in eachcol(z)
        p = NamedTuple([:α, :β, :γ, :δ] .=> parr[2:5])
        u0 = parr[6:7]
        sol_p = solve(prob, Tsit5(); u0, p, saveat)
        plot!(sol_p; alpha=0.1, color="#BBBBBB")
    end

    # Plot simulation and noisy observations.
    plot!(sol; color=[1 2], linewidth=1)
    scatter!(sol.t, data_mat'; color=[1 2])
    return myplot
end

plot_predictions_vi(q, sol, data_mat)

```

- https://turinglang.org/docs/tutorials/09-variational-inference/

### Summary of resources Resources
- https://turinglang.org/docs/tutorials/10-bayesian-differential-equations/
- https://turinglang.org/docs/tutorials/09-variational-inference/
