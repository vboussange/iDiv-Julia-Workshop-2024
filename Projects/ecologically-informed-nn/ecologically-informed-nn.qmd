---
title: "Ecologically-informed neural networks"
jupyter: julia-1.10
execute: 
    eval: false
    cache: true
    warning: false
showsol: false
format:
    html:
        toc: true
        toc-depth: 4
        # toc-expand: 2
editor:
  render-on-save: true
---

![](https://files.ipbes.net/ipbes-web-prod-public-files/styles/xl_1920_scale/azblob/2023-09/IAS%20website%20image.png.webp?itok=tVaiqu2q)

Invasive species pose major threats on nature, natureâ€™s contributions to people, and good quality of life ([IPBES, 2019](https://zenodo.org/records/3553579)), with global annual costs estimated to exceed 423 billion USD. 
The management of invasive alien species requires the modelling of the species range and spread dynamics, in order to optimally allocate control effort.
While current state-of-the-art modelling approaches such as deep learning species distribution model (deep SDM) can help in determining the potential niche of invasive alien species (e.g., [Brun et al. 2024](https://www.nature.com/articles/s41467-024-48559-9)), they assume that species are always in equilibrium with their environment, neglecting key transient ecological processes such as demographic changes and dispersal. This oversight is problematic, because these processes are crucial in determining the establishment of a species in a novel environment.

The proposed project aims to fill this gap by developing an ecologically-informed neural network, following a physics-informed neural network approach (see e.g.[Raissi et al. 2019](https://www.nature.com/articles/s41467-024-48559-9)). 

![](https://benmoseley.blog/wp-content/uploads/2021/08/pinn-1536x608.png)

Specifically, we will build a process-based dynamical model, that we will couple to a deep SDM. The resulting approach will allow to inform the process-based model with presence-absence data, generating a data-driven dynamical forecast of species invasion. 

Julia is great for physics-informed approaches, because it allows to quickly build fast process-based models. Because Julia is an AD-pervasive language, these process-based models can be further differentiated, so that you could eventually calibrate its parameters against the ecological data at hand. 


## What you will learn
- Building a simple deep SDM
- Building a simple process-based model
- Scientific machine learning

## Prerequisite
- [Read this tutorial to understand what is a physics-informed neural network and how to code it in Julia](https://book.sciml.ai/notes/03-Introduction_to_Scientific_Machine_Learning_through_Physics-Informed_Neural_Networks/)
- [Have a look at this Lux tutorial on how to train a MNIST classifier](https://lux.csail.mit.edu/dev/tutorials/beginner/4_SimpleChains)

## Work packages

The different work packages below can be addressed independently and distributed among different teams. The team could collaborate on a public repository.

### **WP A**: Process-based model
The goal is to construct a model which simulates the dynamics of species abundance on a landscape. Given a landscape consisting of $P$ patches $\{p_1, \dots, p_P \}$, we want to model the dynamics of the population abundance vector ${\bf n}_{t} = (n_{1, t}, \dots, n_{P, t})$ where $n_{i, t}$ corresponds to the population density on $p_i$.

We will assume that the population experiences a logistic growth, and that it is subject to dispersal. We will use a difference equation model to express these processes

$$
n_{i, t+1} = n_{i, t} + \mathcal{E}(n_{i, t}) + [\mathcal{D}({\bf n_t})]_i, 
$$

where $\mathcal{E}$ coresponds to the ecological dynamics, 
$$
\mathcal{E}(n_{i, t}) = n_{i, t}(1 - \frac{n_{i, t}}{K_i})
$$
with $K_i$ the carrying capacity on patch $p_i$, and where
$\mathcal{D}$ is the dispersal kernel
$$
[\mathcal{D}({\bf n_t})]_i = \left[ \sum_j  u_{j, t} m_{j, i} - \sum_j  u_{i, t} m_{i, j}\right]
$$

with  $m_{j,i}$ the proportion of individuals migrating from $p_j$ to $p_i$. The first term corresponds to incoming individuals, the second to leaving individuals. 

#### Ecology
We will assume that 
$$
K_{i} = e^{-\kappa(T_i - T^\star)^2}
$$
where $T_i$ is the temperature on patch $p_i$, $T^\star$ is the optimal temperature for the species, and $\kappa$ defines how the carrying capacity decreases under sub-optimal conditions.

#### Dispersal
We will assume that 
$$
m_{i,j} = \begin{cases} 
m \, e^{-d_{i,j}/\alpha}, & \text{if } i \neq j \\
0, & \text{if } i = j 
\end{cases}
$$
where $m$ is the migration rate and $d_{i,j}$ is the distance between $p_i$ and $p_j$.


::: {.callout-warning icon="false"}
# Your task

Implement this model! For this, you'll need to create a `DynSDM` struct. This struct should be used with the `simulate` function:


```julia
model = DynSDM(...)
simulate(model, u0, ntsteps)
```

Given a `model` and an initial abundance vector `u0` and the number of time steps `ntsteps`, this function returns a vector of size `ntsteps` containing abundance vectors with the same shape as `u0`  for each of the time steps investigated.

The dynamics should look as follows:

![](data/anim.gif)


:::

#### Hints
You are given a `utils.jl` file, which contains helper functions to aid your work. Important functions are detailed in the boxes below.

::: {.callout-tip collapse="true"}
#### Loading the data 

You will use temperature data from CHELSA. Run the following snippet. It will download the `bio1` raster from CHELSA, and load it.

```{julia}
include("project_src/utils.jl")

temp = load_raster()

plot(temp)
```
:::

::: {.callout-tip collapse="true"}
# Defining the landscape
The `utils.jl` file provides you with a `Landscape` struct. This struct is very useful, because it calculates the distance between all pixels of a raster. Use it in your model!

Here is a snippet of how you can use it:


```{julia}
landscape = Landscape(temp)
landscape.dists
```
Here, `landscape.dists[i, j]` corresponds to the `d_{i,j}` coefficient, i.e. the distance between pixel `i` and `j`.


The method `get_raster_values` allows you to transform a flat vector into a raster corresponding to the associated landscape. 

For illustration purposes, here we plot the distance of all patch to the 400th patch in the landscape:

```{julia}
distances_to_400 = get_raster_values(landscape.dists[400,:], landscape)
plot(distances_to_400)
```
:::

::: {.callout-tip collapse="true"}
# Numerical values

```{julia}
T0 = 4. # optimal growth temperature
Îº = 1.
Î± = 4. # mean dispersal distance
```
:::


::: {.callout-tip collapse="true"}
# Initial conditions

We suggest using the following initial conditions:

```{julia}
x = lookup(temp, X)
y = lookup(temp, Y)
x_0 = x[floor(Int, length(x)/2)]
y_0 = y[floor(Int, length(y)/2)]
a = 2.
b = 1.
u0_raster = @. exp(- a * ((x - x_0)^2 + (y-y_0)^2)) * exp(- b * (temp - T0)^2)
plot(u0_raster)
```
:::


::: {.callout-tip collapse="true"}
# Defining state variables

It will be easier to handle a flat vector for calculating the dynamics.

```{julia}
u0 = u0_raster[:]
```

But you can use the following helper to transform `u0` or any state variable vector into a raster

```{julia}
plot(get_raster_values(u0, landscape))
```
:::

::: {.callout-tip collapse="true"}
# Dispersal kernel

Implementing the dispersal kernel is not trivial - you need to do it write to get performance. Consider using the following implementation:

```{julia}

disp_proximities = sparse(disp_kern.(landscape.dists)) # we transform the distances in proximities
D = - m * (I - (disp_proximities ./ sum(disp_proximities,dims=1)))
```
Now you need to convince yourself that `D * u` exactly equals $\mathcal{D}({\bf n}_t)$. 
:::

::: {.callout-tip collapse="true"}
# `DynSDM` specs

You probably want to define a structure of the sort 

```julia
Base.@kwdef struct DynSDM{LD,KK,DD} <: AbstractDynModel
    landscape::LD
    K::KK
    D:DD
end
```

that can be instantiated with a function 
```julia
build_dyn_model(landscape::Landscape)
```

This structure will ensure easy access to `K` and `D`, while storing the landscape so that you can project flat state variable vectors to a raster.

:::

<!-- 
You could consider any sort of model, but here are some examples
- The discrete reaction diffusion model from [Bonneau et al. 2017](https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecs2.1979), where individuals experience a logistic growth with a locally defined carrying capacity $K_i$ that depends on the landscape characteristics, and where some individuals disperse to neighboring vertices.
- The metapopulation model from [Hanski et al. 2003](https://www.nature.com/articles/35008063)  -->


### **WP B**: Constructing a deep SDM in Julia
The goal is to construct a deep SDM $\text{NN}$ which can predict the species distribution at time $t$, given an array of absence-presence observations.

$$
y = \text{NN}(\text{long},\text{lat},E,t)
$$

Because the range of the population considered is evolving through time, you want to use spatial location $x$ and $y$ as predictors within the SDM. $E$ corresponds to environmental predictors. We'll only use `bio1` from CHELSA, for the sake of simplicity. Finally, you also want to have the time $t$ in your predictors - we are modelling the spatio-temporal evolution of the species range.

::: {.callout-warning icon="false"}
# Your task

You are given a synthetic presence absence dataset (see green box below.) Build a `Lux.jl` model that predicts the species distribution over the whole spatial domain defined by `raster = load_data()`, for each time steps in the dataset.

<!-- You'll need to build a function 
```julia
train(;nn, 
        PA_data, 
        optim, 
        n_epochs)
``` -->
:::


#### Hints
You are given a `utils.jl` file, which contains helper functions to facilitate your work. Important functions are detailed in the boxes below.

::: {.callout-tip collapse="true"}
# General idea
The script should contain the following functions:

- `build_model`: returns a `Lux` model
- `preprocess_data`: normalizes the raw data for use with the neural network
- `loss`: the loss function
- `train`: the main function, which should contain a loop iterating over batches to update the model parameters
:::


::: {.callout-tip collapse="true"}
# Loading and pre-processing the absence-presence data
You'll work with a synthetic presence-absence (PA) dataset of a virtual invasive species, located at `data/PA_data.csv`. This PA dataset is derived from a true abundance dataset, generated by a process-based model.

- Load the `.csv` as a `DataFrame`.
- Write a `process_data(df; train_split=0.9, batchsize=128)` function which pre-processes the dataframe by normalizing the features, and returns a `train_dataloader` and a `test_dataloader` over which you can iterate. If `x_train` is your training data, its first dimension should be of size 4 `(:x, :y, :temp, :t)`, while its second dimension size should correspond to the batch size.

You may want to use `splitobs` and `DataLoader` `from MLUtils.jl`. You can additionally use the `Standardizer` struct from `MLJ` to standardize your data.
<!-- But feel free to work with real data if you have an idea! -->

<!-- ![](data/PA.gif) -->
<!-- 
You may want to create a function that processes this dataframe so that
1. the resulting processed data is normalized 
2. you can iterate over it -->

<!-- Notice how the number of samples increases over time. You should correct for this bias! -->

<!-- ### True population abundance data
![](data/anim.gif)
in `data/true_abundance_data.jld2`

You may use this dataset to evaluate your model, but not for training! -->
:::


::: {.callout-tip collapse="true"}
# ðŸ†˜ Loading and pre-processing the absence-presence data
- To normalize `data` (a DataFrame), you can use
```julia
standardizer = machine(Standardizer(), data)
fit!(standardizer)
data = (MLJ.transform(standardizer, pred_data) |> Array)' .|> Float32
```

- `splitobs` may be used as such:

```julia
(x_train, y_train), (x_test, y_test) = splitobs((x_data, y_data); at=train_split)
```
-  a `DataLoader` can be defined as such

```julia
DataLoader(collect.((x_train, y_train)); batchsize, shuffle=true)
```

:::


::: {.callout-tip collapse="true"}
# Neural network architecture and loss
Use `Lux.Chain` to create a MLP with `Dense` layers. You may want to use a `BatchNorm` layer to accelerate the training. The last activation function should be `Lux.sigmoid`, returning a probability `yÌ‚`. This probability should be matched against the boolean absence-presence variable `y` with `Flux.binarycrossentropy`.

:::

::: {.callout-tip collapse="true"}
# ðŸ†˜ Neural network architecture and loss
- Consider using the following function to create your model.

```julia
function build_model(; npred, hidden_nodes, hidden_layers, dev=cpu)
    model = Chain(Dense(npred, hidden_nodes),
        BatchNorm(hidden_nodes),
        [Dense(hidden_nodes, hidden_nodes, tanh) for _ âˆˆ 1:hidden_layers-1]...,
        Dense(hidden_nodes, 1),
        Lux.sigmoid
    )

    return model
end

```
- Observe that `loss` should have the signature `loss(model, ps, st, data)` to be a valid function to be trained with the `Lux.Training` API (see box below):
```julia
Training.single_train_step!(AutoZygote(), loss, data, train_state)
```
- `loss` should return a tuple: `l, st, (;)`, with `l` the loss value and `st` the state of the model
- Remember how to evaluate a `Lux` model:
```julia
yÌ‚, st = Lux.apply(model, x, ps, st)
```
:::

::: {.callout-tip collapse="true"}
# Training your network
Wihin the `train` function, you want to create a `TrainState`
```julia
train_state = Training.TrainState(rng, model, opt)
```
where `opt` could be the Adam optimizer
```julia
using Optimisers
opt = Adam(0.01)
```
and `rng` is a random number generator
```julia
rng = Random.default_generator()
```

You also probably want to have a loop similar to 

```julia
for data in train_dataloader
    (_, l, _, train_state) = Training.single_train_step!(AutoZygote(), loss, data, train_state)
end
```
which will update the model parameters for each batch, and an outer loop to iterate multiple times over your data (epochs):

```julia
for epoch in 1:n_epochs
# ...
end

```
You eventually wan to throw the trained parameters and state of the model:

```
(train_state.parameters,  Lux.testmode(train_state.states))
```
:::


::: {.callout-tip collapse="true"}
#### Projecting the model's predictions spatially

To project the deep SDM spatially, you'll need the `temp` raster, that you can obtain by running the following snippet. This will download the `bio1` (mean annual temperature) raster from CHELSA, and load it.

```{julia}
include("project_src/utils.jl")
temp = load_raster()
plot(temp)
```

- With this raster, you want to construct a dataframe which contains `temp` as a tabular data, together with associated longitude and latitude. You finally want to append to this dataframe the time.
- With this dataframe, you can predict a flat vector of abundance, which you can map back to a raster with the helper function `get_raster_values`:
```julia
yÌ‚, _ = Lux.apply(nn, pred_all, ps, st)
predicted = reshape(yÌ‚, :)
urast = get_raster_values(predicted, landscape)
```

<!-- Feel free to use additional predictors, by modifying the `load_raster` function. -->

:::

::: {.callout-warning icon="false"}
# Optional task: Hyperparameter optimisation

Hyperparameters of a neural network model are parameters that are not learned during training but are set by the user before training. The hyperparameters define the structure and behavior of the neural network and influence how the model learns from the input data. 
Some resources:
- [Hyperopt.jl](https://github.com/baggepinnen/Hyperopt.jl).

- [A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)


:::

#### **WP C**: Ecologically-informed neural network
The objective is to constrain the deep SDM developed in **B** with the process-based model developed in **A**.

Specifically, you should additionaly constrain the deep SDM so that its predictions are coherent with the process-based model. That is, we eventually want that

`simulate(model, nns[i-1](pred), 1) â‰ˆ nns[i](pred)`

while `nns` still predict the PA data. To do so, build a function 
```julia
train(;nns, 
        dyn_model, 
        PA_data, 
        optim, 
        n_epochs,)
```

to perform training.


Once this is done, check how good are your predictions outside the training range by predicting with

```julia
simulate(model, f[end](pred), 1)
```

which will predict ${\bf n}_{T+1}$.


